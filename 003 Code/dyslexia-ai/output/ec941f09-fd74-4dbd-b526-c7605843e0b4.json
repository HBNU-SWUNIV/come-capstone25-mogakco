{
  "jobId": "ec941f09-fd74-4dbd-b526-c7605843e0b4",
  "fileName": "fine-tuning.pdf",
  "status": "COMPLETED",
  "createdAt": "2025-09-18T00:15:43.279654",
  "completedAt": "2025-09-18T00:20:01.637809",
  "processingTimeSeconds": 258.358155,
  "metadata": {
    "model": "claude-sonnet-4-20250514",
    "total_pages_from_pdf": 10,
    "total_chunks": 2,
    "total_blocks": 137
  },
  "pages": [
    {
      "page_number": 1,
      "original_content": "Fine-tuning CLIP Text Encoders with Two-step Paraphrasing HyunjaeKim1 SeunghyunYoon2 TrungBui2 HandongZhao2 QuanTran2 FranckDernoncourt2 JaewooKang1 1KoreaUniversity 2AdobeResearch {hyunjae-kim,kangj}@korea.ac.kr {syoon,bui,hazhao,qtran,dernonco}@adobe.com Abstract (Top-3) Retrieved Images by CLIP Contrastive language-image pre-training (CLIP) models have demonstrated consider- able success across various vision-language tasks,suchastext-to-imageretrieval,wherethe Query A: A newlymarriedcouple walkingdown a street. modelisrequiredtoeffectivelyprocessnatural language input to produce an accurate visual output. However, current models still face limitationsindealingwithlinguisticvariations ininputqueries,suchasparaphrases,making itchallengingtohandleabroadrangeofuser QueryB: A recentlywedcouple strollingdown a road. queriesinreal-worldapplications. Inthisstudy, Figure1:ImageretrievalresultsofCLIP(Radfordetal., we introduce a straightforward fine-tuning 2021)fortwodifferentqueries(thegoldimageisde- approach to enhance the representations of notedbyaboldborder).Despitetheircomparablemean- CLIP models for paraphrases. Our approach ings,themodelyieldsdissimilarretrievalresults,high- involves a two-step paraphrase generation lightingthemodelâ€™sstrugglewithlinguisticvariations. process, where we automatically create two categories of paraphrases from web-scale image captions by leveraging large language Aninherentchallengeinvision-languagetasks models. Subsequently, we fine-tune the lies in the variability of text inputs. Even when CLIP text encoder using these generated paraphraseswhilefreezingtheimageencoder. conveying similar meanings and intentions, they Ourresultingmodel,whichwecallParaCLIP, canexhibitvariationsinvocabularyandstructure exhibitssignificantimprovementsoverbaseline dependingontheparticularuser. Consequently,it CLIP models across various tasks, including becomescrucialtoensurethatCLIPâ€™stextencoders paraphrased retrieval (with rank similarity arerobustenoughtohandlediversesynonymsand scores improved by up to 2.0% and 5.6%), paraphrases in practical scenarios. However, cur- Visual Genome Relation and Attribution, as rent text encoders exhibit limited proficiency in wellassevensemantictextualsimilaritytasks. comprehending linguistic variations, resulting in differentretrievalresultsforuserquerieswithsimi- 1 Introduction larmeanings(Figure1).\n\nContrastive language-image pre-training (CLIP) To address this challenge, we introduce a models(Radfordetal.,2021)havegainedsignifi- straightforwardmethodtoimproveCLIPâ€™stexten- cantattentioninthefieldsofcomputervisionand coders. Specifically,wegeneratedtwocategories naturallanguageprocessingfortheirremarkableca- of paraphrases for image captions sourced from pacitytounderstandtherelationshipbetweentext the web, leveraging recent large language mod- andimages. Theyhavebeenwidelyusedinvarious els(LLM)suchasChatGPT(OpenAI,2022)and vision-languageapplications,includingimageclas- LLaMA(Touvronetal.,2023). Subsequently,we sification(Dengetal.,2009),imageretrieval(Lin utilized image captions and their corresponding et al., 2014; Plummer et al., 2015), and text-to- paraphrases to fine-tune the text encoder, which imagegeneration(Sahariaetal.,2022;Rombach ensures that the representations of captions and etal.,2022),wherethemodelshouldreturndesired paraphrasesclusterinasimilarvectorspace. visualoutputsforagiventext,andviceversa. Wevalidatedtheeffectivenessofourapproach 4202 beF 32 ]VC.sc[ 1v02151.2042:viXra\n\nImage ( ) Caption ( ): Reversible Cake Plate / Chip &Dip -Christmas Mistletoe ğ’™ğ’™ğˆğˆ ğ’™ğ’™ğ“ğ“ Step 1. Caption-to-paraphrase Generation LLM Prompt: Paraphrase the given text â€œ{text}â€ concisely while preserving the meaning.\n\nParaphrase ( ): Christmas Mistletoe ReversibleCake Plate and Chip &Dip â€² ğ’™ğ’™ğ“ğ“ Step 2. Paraphrase-to-paraphrase Generation LLM Prompt: Paraphrase the given text â€œ{text}â€ concisely while preserving the meaning and avoiding use of existing words.\n\nParaphrase ( ): A cake plate and chip &dip set that can be flipped overfor Christmas mistletoe design. â€²â€² ğ’™ğ’™ğ“ğ“ Figure 2: Overview of our two-step paraphrasing process. (1) In caption-to-paraphrase generation, the first paraphraseisgeneratedbyremovingnoisefromtheoriginalcaptionandconvertingitintoamoreplainlanguage. (2)Inparaphrase-to-paraphrasegeneration,thesecondparaphraseisgeneratedfromthefirstparaphrase,wherethe wordâ€œreversibleâ€ischangedtoasemanticallysimilarexpressionâ€œcanbeflippedover.â€ using evaluation tasks that assess modelsâ€™ under- phrasesthroughatwo-stepparaphrasingprocess, standingoflanguagesemanticsandcomposition: caption-to-paraphrasegenerationandparaphrase- paraphrased retrieval, Visual Genome Relation to-paraphrasegeneration,asillustratedinFigure2. (VG-R),VisualGenomeAttribution(VG-A)(Yuk- Caption-to-paraphrase generation This pro- sekgonuletal.,2023), andsemantictextualsimi- cess directly rewrites original captions. Image larity(STS)tasks(Agirreetal.,2012). Ourmod- captions on the web often contain considerable els,ParaCLIP,significantlyoutperformedbaseline noise, such as superfluous punctuation, product CLIPmodels,whilemaintainingorsometimesim- codes,andfileextensions,whichdifferfromtypi- provingitsrobustperformanceonzero-shotimage calqueries. Thisstepcanbeseenasresponsiblefor classification(Dengetal.,2009),aswellastextand convertingthesenoisycaptionsintoamorestraight- image retrieval (Lin et al., 2014). We emphasize forward text format commonly used in everyday thatthisisthefirststudytoimprovetherepresenta- language. Using the power of LLMs, we synthe- tionsofCLIPâ€™stextencodersduringthefine-tuning sized paraphrases xâ€² for each caption with the stageusingsyntheticparaphrases. T followingprompt: â€œParaphrasethegivencaption 2 Method â€œtextâ€conciselywhilepreservingthemeaning.â€, wheretextissubstitutedwithagivencaption.\n\nOurobjectiveistorefinetheCLIPmodelâ€™straining process,enablingitstextencodertoproducecon- Paraphrase-to-paraphrasegeneration Inthis sistentrepresentationsforvarioussemanticallysim- step, additional paraphrases, xâ€²â€², are generated T ilartextualinputsthatthemodelmightencounter for each generated paraphrase, xâ€² . The para- T inreal-worldscenarios. Certainimage-captioning phrasing process is similar to the previous step, datasetsprovidemultiplecaptionsforasingleim- but with some differences in the prompt as age(Linetal.,2014;Plummeretal.,2015),which follows: â€œParaphrase the given text â€œtextâ€ mightbeutilizedassemanticallysimilartextpairs concisely while preserving the meaning and during training. However, the volume of these avoidinguseofexistingwords.â€,wheretheunder- datasets is limited, which presents a challenge in linedtextisusedtopromptthemodeltoproduce termsofexposingmodelstodiverselanguagepat- morphologicallydiverseexpressions. terns. Therefore, we automatically generated se- 2.2 TrainingObjectives manticallysimilarpairs(i.e.,paraphrases)formil- lionsofimagecaptionssourcedfromtheweb. LetX I ,X T ,Xâ€² T ,andXâ€² T â€² bemini-batchesofN ex- amplesofanimagex ,captionx ,andtwotypes I T 2.1 ParaphraseGeneration of paraphrases, xâ€² and xâ€²â€². The final loss is cal- T T Animage-captioningdatasettypicallycomprisesa culated as the summation of three sub-losses as collectionofimage-captionpairs(x ,x ),where follows: L := L (X ,Xâ€²â€²)+L (X ,Xâ€² )+ I T total 1 I T 2 T T x and x represent an image and the corre- L (Xâ€² ,Xâ€²â€²).Thefirstterm,L ,representstheIn- I T 3 T T 1 sponding caption, respectively. For each cap- foNCElossfunctionthatoperatesbetweenimages tion x , we created two categories of para- andtext(Oordetal.,2018). Thislossfunctionis T\n\ncrucialinthepreventionofforgettingCLIPâ€™srep- coderwasinitializedwiththeweightsofRoBERTa- resentations and knowledge acquired during pre- base (Liu et al., 2019) for better linguistic com- training. Weusedtheparaphrasedversionoftext prehension capabilities. (4) LaCLIP (Fan et al., inputXâ€²â€² ratherthantheoriginalcaptionsX be- 2023) was pre-trained using the LAION-400m T T causeuserqueriesoftenresembleplaintextrather dataset augmented with automatically generated than the original captions. This choice led to im- paraphrases.2 Specifically,asmallnumberoforig- provedperformanceonthebenchmarkdatasetsdur- inal caption and paraphrase pairs were obtained ing our preliminary experiment. If the target do- fromCOCOtextdescriptions,orcreatedbyChat- maininvolvesdealingwithnoisytextinputs,such GPT,GoogleBARD,andhumans. Theseseedex- asinanonlineshoppingmallcontext,employing ampleswereusedtopromptanLLaMA7Bmodel theoriginalcaptionsmaybemoreeffective. throughain-contextlearningapproach,whichthen Thesecondterm,L ,accountsfortherelation- generatedparaphrasesfortheentireLAION-400m 2 shipbetweencaptionsandtheirparaphrases. Con- dataset. Duringpre-training,astandardInfoNCE ceptually,itservestoestablishaconnectionwithin loss was computed using these paraphrases and the vector space between the representation of correspondingimagesincombinationwithoriginal noisycaptionsandtheplaintextcommonlyused captionandimagepairs. Whileourmethodshares in everyday language. Lastly, L serves to bring somesimilaritieswithLaCLIPintheuseofmodel- 3 together various semantically similar plain texts generatedparaphrases,itshouldbenotedthatours within a vector space. For L and L , we used has unique advantages. First, we enhance CLIP 2 3 theInfoNCEloss. TheresultingCLIPmodelfine- modelsthroughfine-tuningthetextencoderswhile tunedusingthesethreelossesiscalledParaCLIP. freezingtheimageencoders,whichissignificantly moreefficientcomparedtopre-trainingtheentire 3 ExperimentalSetups model from scratch. Despite its efficiency, our methodissignificantlymoreeffectivetoimprove We obtained image-caption pairs using LAION- the CLIPâ€™s robustness to paraphrases, improving 400M (Schuhmann et al., 2021). We initially theperformanceinparaphrasedretrievalbyalarge generated 300K paraphrases using ChatGPT and margin(seeSection4fordetails). instruction-tuned an open-sourced LLM named LLaMA (7B) (Touvron et al., 2023) using these 3.2 Evaluation 300Kdatatogenerateadditionalparaphrases.1 Our final dataset comprises 5M examples of x , x , We evaluated models on the following tasks in I T xâ€² ,andxâ€²â€². Moredetailsandhyperparametersare a zero-shot manner, without fine-tuning them on T T describedinAppendixA. thetargettasks. (1)Paraphrasedretrieval(Cheng etal.,2024)involvesretrievingidenticalimagesfor 3.1 BaselineModels both 4,155 original queries and their correspond- ingparaphrasesfromtheimagesetoftheCOCO We used the following CLIP models as base- 2017validationset(Linetal.,2014). Paraphrases line models, all built upon the ViT-B/32 archi- weregeneratedusingGPT-3(Brownetal.,2020) tecture (Dosovitskiy et al., 2021). (1) OpenAIâ€™s and subsequently verified by humans. This task CLIP (Radford et al., 2021) was trained using is well-suited for assessing modelsâ€™ ability to ef- a private dataset comprising 400M image-text fectivelyhandleuserqueriesexpressedindiverse pairs sourced from the web. (2) OpenCLIP forms. For metrics, we used the top-10 average models (Cherti et al., 2023) were trained us- overlap(AO@10)andJaccardsimilarity(JS@10) ing the largest open-sourced datasets, LAION- scores,whichmeasurethedegreeofranksimilarity 400M and LAION-2B (Schuhmann et al., 2022). betweenthetop10imagesretrievedfortheoriginal (3) OpenCLIP-RoBERTa was pre-trained using queryandparaphrasedquery. Detaileddescriptions LAION-2B.Incontrasttotheusualpracticewhere ofthemetricscanbefoundinAppendixB. textencodersareinitializedwithrandomweights andsubsequentlytrainedfromscratch,itstexten- (2) VG-R and (3) VG-A (Yuksekgonul et al., 2023)aredevisedtoassessrelationalandattribu- 1WeverifiedthatthedatageneratedbyLLaMAexhibited tiveunderstandingofvision-languagemodels,re- comparablequalitytothatofChatGPT.Additionally,when spectively. They involve determining the correct trainingthemodelusing300KparaphrasesfromLLaMAand anadditional300KparaphrasesfromChatGPT,respectively, weobservedsimilarperformanceinbothcases. 2https://github.com/LijieFan/LaCLIP\n\nParaphrasedRtrv. VG-R VG-A STS Clsf. TRtrv. IRtrv.\n\nModel AO@10 JS@10 Acc Acc Avg. Acc R@5 R@5 OpenAIâ€™sCLIP(400M) 67.2 57.7 59.7 63.2 65.1 63.4 75.0 54.8 +ParaCLIP 72.2 63.3 60.7 64.3 72.2 63.5 77.0 58.8 OpenCLIP(400M) 67.6 58.9 46.4 57.8 67.2 60.2 76.5 59.4 +ParaCLIP 71.3 62.9 55.4 61.7 70.1 60.8 76.1 59.4 OpenCLIP(2B) 70.6 62.1 45.0 61.8 69.6 66.5 80.2 64.8 +ParaCLIP 73.2 65.1 58.8 65.4 71.6 65.5 80.4 63.3 OpenCLIP-RoBERTa(2B) 72.5 64.0 35.6 64.5 71.0 61.8 78.8 62.6 +ParaCLIP 74.5 66.2 43.2 66.5 72.5 61.4 79.4 62.0 LaCLIP(400M) 69.9 62.1 50.6 63.6 58.8 64.5 68.1 55.5 +ParaCLIP 73.5 65.8 60.6 64.6 71.4 64.5 73.6 58.0 Table1: Zero-shotperformanceofbaselineCLIPmodelsandourParaCLIPmodels. Thebestscoresarerepresented inbold. â€œAccâ€: Accuracy. â€œAvg.â€: MacroaverageofSpearmanâ€™srankcorrelationsacrossallSTStasks. â€œClsf.â€: Imageclassification. â€œTRtrv.â€: Textretrieval. â€œIRtrv.â€: Imageretrieval. captionforagivenimagefromtwocandidatecap- Effectoffine-tuningusingparaphrases Across tions, where negative captions are generated by allCLIPmodels,ourapproachconsistentlydemon- interchangingobjectsbasedontheirrelationalcon- stratedimprovedperformanceinthefourprimary textorinterchangingattributesofobjects. Forin- tasks. Notably,themostsignificantimprovements stance,giventhecorrectcaptionâ€œthedogisbehind were observed in the paraphrased retrieval task, the tree,â€ a negative counterpart could be formu- where our ParaCLIP model achieved 72.2% and latedasfollows: â€œthetreeisbehindthedog.â€ The 63.3% in AO@10 and JS@10 scores, increasing VG-R and VG-A datasets comprise 23,937 and the performance of OpenAIâ€™s CLIP by 5.0% and 28,748testexamples,respectively. 5.6%,respectively.3 TheimprovementsintheSTS (4)STShasbeenwidelyemployedtoevaluate tasksarealsonoticeable,withthemacro-average thetextrepresentationsofencoders(Conneauetal., scoreimprovingby7.1%. Althoughnotinallcases, 2017;ReimersandGurevych,2019;Chuangetal., our approach generally enhances performance in 2022). Thistaskinvolvesmeasuringsemanticsim- the text retrieval task. This is attributed to our ilarity or relatedness between pairs of text. Fol- modelâ€™s capability to encode texts that shares se- lowingGaoetal.(2021),wemeasuredSpearmanâ€™s manticsimilaritywithagiveninputimageclosely correlationforeachtaskintheâ€œallâ€aggregationset- withinthevectorspace. tingandreportedmacro-averagedscoresacrossthe Effect of initialization with RoBERTa The sevenSTStasks(Agirreetal.,2012,2013,2014, OpenCLIP-RoBERTamodelsignificantlyoutper- 2015,2016;Ceretal.,2017;Marellietal.,2014). formedtheOpenCLIP(2B)modelinparaphrased Additionally, we assessed whether our models retrievalandSTS,highlightingthebenefitsoflever- can maintain or even improve their performance agingpre-trainedlanguagemodelsoverrandomly on standard vision or vision-language tasks after initializedtextencoders. However,evenwiththese beingfine-tuned,includingzero-shotimageclassi- advancements, there is substantial room for im- ficationontheImageNet-1Kvalidationset(Deng provementinperformanceonthesetasks. Ourfine- etal.,2009), andimage-to-textretrievalandtext- tuningapproachfurtherrefinedtheRoBERTatext to-imageretrievalontheCOCOvalidationset(Lin encoder, leading to notable achievements across etal.,2014). Formetrics,top-1accuracy(Acc)and the four primary tasks, with 2.0% (AO@10) and top-5recall(R@5)wereusedintheclassification 2.2%(JS@10)scoresinparaphrasedretrieval. andretrievaltasks,respectively.\n\nComparison with LaCLIP While LaCLIP ex- 4 ResultsandDiscussion hibited superior performance compared to the OpenCLIP(400M)modelinimageclassification, 4.1 MainResults paraphrased retrieval, VG-R, and VG-A, its per- Table 1 shows the zero-shot performance of the 3AcasestudycomparingCLIPandParaCLIPinthepara- baselineandourmodelsintheevaluationtasks. phrasedretrievaltaskcanbefoundinAppendixC.\n\nParaphrasedRtrv. VG-R VG-A STS Clsf. TRtrv. IRtrv.\n\nModel AO@10 JS@10 Acc Acc Avg. Acc R@5 R@5 OpenAIâ€™sCLIP(400M) 67.2 57.7 59.7 63.2 65.1 63.4 75.0 54.8 +L 68.9 59.9 58.0 62.4 68.7 63.7 75.8 58.0 1 +L +L 70.5 61.2 61.5 65.1 74.5 56.7 74.6 51.8 2 3 +L +Lâ€² 70.4 61.7 58.2 63.0 69.1 64.0 76.3 58.7 1 1 +L +Lâ€² +Lâ€²â€² 71.3 62.8 58.9 63.4 68.8 64.1 76.4 58.8 1 1 1 +L +L 69.1 60.0 59.1 63.3 71.8 63.5 76.1 58.2 1 2 +Lâ€² +L 70.8 62.0 60.4 64.0 71.6 63.7 76.4 58.6 1 2 +L +L +L 69.6 60.5 59.2 63.4 72.4 63.1 76.4 58.1 1 2 3 +Lâ€²â€²+L +L (Ours) 72.2 63.3 60.4 64.2 72.2 63.5 77.0 58.8 1 2 3 Table2: Zero-shotperformanceofOpenAIâ€™sCLIP(400M)withdifferentlossfunctionsapplied. Thebestscoresare representedinboldandthesecondbestscoresareunderlined. â€œParaphrasedRtrv.â€: Paraphrasedretrieval. â€œAccâ€: Accuracy. â€œAvg.â€: MacroaverageofSpearmanâ€™srankcorrelationsacrossallSTStasks. â€œClsf.â€: Imageclassification. â€œTRtrv.â€: Textretrieval. â€œIRtrv.â€: Imageretrieval. formanceinthetext/imageretrievalandSTStasks ourParaCLIPmodelâ€™sperformance. WhenLâ€²â€² was 1 witnessedadecline. Thisindicatesthataugmenting omitted (i.g., L + L ), the model showed the 2 3 paraphrased text data may not consistently yield best performance on the VG-R, VG-A, and STS improvements,withoutincorporatingeffectiveloss tasks,buttheperformanceonimageclassification functionssuchasL andL . Conversely,ourfine- andstandardtextandimageretrievalsignificantly 2 3 tuning method dramatically enhanced LaCLIPâ€™s degraded. This indicates that Lâ€²â€² was crucial in 1 performance in paraphrased retrieval (+ 3.6% in preserving the representations of CLIP acquired AO@10 and 3.7% in JS@10), VG-R (+ 10.0%), during pre-training. Although simply augment- VG-A(+1.0%),STS(+12.6%),andevenontext ing training data with synthetic paraphrases (i.e., retrieval (+ 5.5%) and image retrieval (+ 2.5%), L + Lâ€² and L + Lâ€² + Lâ€²â€²) generally led to 1 1 1 1 1 highlightingthatourmethodcancomplementLa- performanceimprovements,theimprovementsin CLIPtoachieveoptimalperformance. the STS tasks were not substantial compared to the models with the L and L losses. Apply- 2 3 Lackofcompositionalunderstanding AllCLIP ingL wasparticularlyeffectiveforSTSbecause 3 modelsexhibitedsignificantdeficienciesintheVG- it involved comparing pairs of semantically sim- R and VG-A tasks. These limitations in compo- ilar â€œplainâ€ text (not pairs of noisy caption and sitionalunderstandingcanleadtoerrorsindown- plaintext),whichalignswellwiththegoalofSTS. stream tasks such as text-to-image synthesis, in- Finally,ourParaCLIPmodel,incorporatingthree cludingunintentionalattributeinterchangesorthe losses(i.e.,Lâ€²â€² +L +L ),showedthemostbal- 1 2 3 omission of objects in generated images (Feng ancedperformanceacrossalltasksamongthevar- etal.,2023). Infutureresearch,weplantoconduct iousmodelsevaluated. Inparticular,applyingLâ€²â€² 1 amorein-depthanalysistoexplorethepotentialof insteadofL provedtobegenerallyeffective. 1 ourapproachtomitigatetheseissues. 5 Conclusion 4.2 AblationStudy Inthisstudy,weproposedatwo-stepparaphrasing We conducted an ablation study to closely exam- approachforenhancingtherepresentationsofCLIP inetheindividualcontributionsofeachlossterm forparaphrasesthatmayoccurintextinputsinreal- (Table 2). In this section, we simplify the nota- tion L (X ,X ), L (X ,Xâ€² ), and L (X ,Xâ€²â€²) world applications. Our ParaCLIP models, fine- 1 I T 1 I T 1 I T toL ,Lâ€²,andLâ€²â€²,respectively. NotethatourPara- tuned using synthetic paraphrases, outperformed 1 1 1 baselinemodelsbyalargemarginonvarioustasks CLIPmodelwastrainedusingthecombinedloss functions,Lâ€²â€²+L +L ,asdetailedinSection2.2. requiring language semantics and compositional 1 2 3 understanding,includingparaphrasedretrieval.\n\nFirst,wefine-tunedtheOpenAIâ€™sCLIPmodel using the same set of image-caption pairs in Limitations LAION-400Masourmodel,excludingparaphrases (referred to as â€œL â€). While there was an overall Ourmethodsometimesdegradestheperformance 1 improvement in performance, it still fell short of of CLIP on conventional vision and vision-\n\nlanguagetaskssuchaszero-shotclassificationand EnekoAgirre,CarmenBanea,DanielCer,MonaDiab, image retrieval. A significant factor contributing Aitor Gonzalez-Agirre, Rada Mihalcea, German Rigau, and Janyce Wiebe. 2016. SemEval-2016 tothisperformancevariationmaybethesensitiv- task 1: Semantic textual similarity, monolingual ity of the infoNCE loss to changes in batch size. andcross-lingualevaluation. InProceedingsofthe We observed consistent improvements in the im- 10thInternationalWorkshoponSemanticEvaluation ageclassificationandtext/imageretrievaltasksby (SemEval-2016),pages497â€“511,SanDiego,Califor- nia.AssociationforComputationalLinguistics. scaling up the batch size from 256 to 3K. Unfor- tunately, due to constraints in computational re- Eneko Agirre, Daniel Cer, Mona Diab, and Aitor sources,wewereunabletomatchthebatchsizeto Gonzalez-Agirre. 2012. SemEval-2012 task 6: A thescaleofCLIPhyperparameters(e.g.,OpenAIâ€™s pilotonsemantictextualsimilarity. In*SEM2012: TheFirstJointConferenceonLexicalandCompu- CLIP was pre-trained using a batch size of 32K). tationalSemanticsâ€“Volume1: Proceedingsofthe Asaresult,theeffectofbatchsizeincausingthe main conference and the shared task, and Volume observed performance degradation has not been 2: ProceedingsoftheSixthInternationalWorkshop thoroughly validated in this study. Although the onSemanticEvaluation(SemEval2012),pages385â€“ 393, MontrÃ©al, Canada. Association for Computa- primarygoalofthispaperwastoshowcasethepo- tionalLinguistics. tential improvements in the CLIP model through synthetic paraphrasing and better generalization EnekoAgirre,DanielCer,MonaDiab,AitorGonzalez- abilityacrossvariousinputqueries,acomprehen- Agirre,andWeiweiGuo.2013. *SEM2013shared task: Semantic textual similarity. In Second Joint sive investigation into the factors contributing to ConferenceonLexicalandComputationalSemantics performance degradation should be conducted in (*SEM),Volume1: ProceedingsoftheMainConfer- futureresearch. enceandtheSharedTask: SemanticTextualSimilar- ity,pages32â€“43,Atlanta,Georgia,USA.Association forComputationalLinguistics.\n\nAcknowledgements Tom Brown, Benjamin Mann, Nick Ryder, Melanie WethankFabianCabaHeilbronandDongheeChoi Subbiah,JaredDKaplan,PrafullaDhariwal,Arvind for their help and insightful discussions. This re- Neelakantan,PranavShyam,GirishSastry,Amanda search was supported by (1) National Research Askell,etal.2020. Languagemodelsarefew-shot learners. Advancesinneuralinformationprocessing FoundationofKorea(NRF-2023R1A2C3004176), systems,33:1877â€“1901. (2)ICTCreativeConsilienceProgramthroughthe InstituteofInformation&CommunicationsTech- Daniel Cer, Mona Diab, Eneko Agirre, IÃ±igo Lopez- Gazpio, and Lucia Specia. 2017. SemEval-2017 nologyPlanning&Evaluation(IITP)grantfunded task1: Semantictextualsimilaritymultilingualand bytheKoreagovernment(MSIT)(IITP-2024-2020- crosslingual focused evaluation. In Proceedings 0-01819),and(3)aKoreaUniversityGrant. of the 11th International Workshop on Semantic Evaluation(SemEval-2017),pages1â€“14,Vancouver, Canada.AssociationforComputationalLinguistics.\n\nReferences JiachengCheng,HijungValentinaShin,NunoVasconce- los,BryanRussell,andFabianCabaHeilbron.2024.\n\nEneko Agirre, Carmen Banea, Claire Cardie, Daniel Adaptingcliptoparaphrasedretrievalwithpretrained Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei languagemodels.\n\nGuo,IÃ±igoLopez-Gazpio,MontseMaritxalar,Rada Mihalcea,GermanRigau,LarraitzUria,andJanyce Mehdi Cherti, Romain Beaumont, Ross Wightman, Wiebe.2015. SemEval-2015task2: Semantictex- MitchellWortsman,GabrielIlharco,CadeGordon, tual similarity, English, Spanish and pilot on inter- ChristophSchuhmann,LudwigSchmidt,andJenia pretability. InProceedingsofthe9thInternational Jitsev. 2023. Reproducible scaling laws for con- WorkshoponSemanticEvaluation(SemEval2015), trastive language-image learning. In Proceedings pages252â€“263,Denver,Colorado.Associationfor of the IEEE/CVF Conference on Computer Vision ComputationalLinguistics. andPatternRecognition,pages2818â€“2829.\n\nEnekoAgirre,CarmenBanea,ClaireCardie,DanielCer, Yung-SungChuang,RumenDangovski,HongyinLuo, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, YangZhang, ShiyuChang, Marin Soljacic, Shang- RadaMihalcea,GermanRigau,andJanyceWiebe. WenLi,ScottYih,YoonKim,andJamesGlass.2022.\n\n2014. SemEval-2014task10: Multilingualsemantic DiffCSE:Difference-basedcontrastivelearningfor textualsimilarity. InProceedingsofthe8thInterna- sentence embeddings. In Proceedings of the 2022 tionalWorkshoponSemanticEvaluation(SemEval Conference of the North American Chapter of the 2014),pages81â€“91,Dublin,Ireland.Associationfor AssociationforComputationalLinguistics: Human ComputationalLinguistics. LanguageTechnologies,pages4207â€“4218,Seattle,\n\nUnited States. Association for Computational Lin- Ilya Loshchilov and Frank Hutter. 2019. Decoupled guistics. weightdecayregularization. InInternationalConfer- enceonLearningRepresentations.\n\nAlexisConneau,DouweKiela,HolgerSchwenk,LoÃ¯c Barrault, and Antoine Bordes. 2017. Supervised MarcoMarelli,StefanoMenini,MarcoBaroni,Luisa learningofuniversalsentencerepresentationsfrom Bentivogli, Raffaella Bernardi, and Roberto Zam- naturallanguageinferencedata. InProceedingsof parelli. 2014. A SICK cure for the evaluation of the2017ConferenceonEmpiricalMethodsinNat- compositional distributional semantic models. In uralLanguageProcessing,pages670â€“680,Copen- ProceedingsoftheNinthInternationalConference hagen,Denmark.AssociationforComputationalLin- onLanguageResourcesandEvaluation(LRECâ€™14), guistics. pages216â€“223,Reykjavik,Iceland.EuropeanLan- guageResourcesAssociation(ELRA).\n\nJiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi, andLiFei-Fei.2009. Imagenet: Alarge-scalehier- AaronvandenOord,YazheLi,andOriolVinyals.2018. archicalimagedatabase. In2009IEEEconference Representationlearningwithcontrastivepredictive oncomputervisionandpatternrecognition, pages coding. arXivpreprintarXiv:1807.03748. 248â€“255.Ieee.\n\nOpenAI.2022. Introducingchatgpt.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Bryan A Plummer, Liwei Wang, Chris M Cervantes, Thomas Unterthiner, Mostafa Dehghani, Matthias Juan C Caicedo, Julia Hockenmaier, and Svetlana Minderer, Georg Heigold, Sylvain Gelly, Jakob Lazebnik. 2015. Flickr30k entities: Collecting Uszkoreit, and Neil Houlsby. 2021. An image region-to-phrasecorrespondencesforricherimage- is worth 16x16 words: Transformers for image to-sentence models. In Proceedings of the IEEE recognitionatscale. InInternationalConferenceon internationalconferenceoncomputervision,pages LearningRepresentations. 2641â€“2649.\n\nRonaldFagin,RaviKumar,andDakshinamurthiSivaku- AlecRadford,JongWookKim,ChrisHallacy,Aditya mar.2003. Comparingtopklists. SIAMJournalon Ramesh,GabrielGoh,SandhiniAgarwal,GirishSas- discretemathematics,17(1):134â€“160. try, Amanda Askell, Pamela Mishkin, Jack Clark, Lijie Fan, Dilip Krishnan, Phillip Isola, Dina Katabi, etal.2021. Learningtransferablevisualmodelsfrom andYonglongTian.2023. Improvingcliptraining naturallanguagesupervision. InInternationalconfer- withlanguagerewrites. AdvancesinNeuralInforma- enceonmachinelearning,pages8748â€“8763.PMLR. tionProcessingSystems.\n\nNils Reimers and Iryna Gurevych. 2019. Sentence- Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, BERT:SentenceembeddingsusingSiameseBERT- Arjun Akula, Pradyumna Narayana, Sugato Basu, networks. InProceedingsofthe2019Conferenceon Xin Eric Wang, and William Yang Wang. 2023. EmpiricalMethodsinNaturalLanguageProcessing Training-freestructureddiffusionguidanceforcom- andthe9thInternationalJointConferenceonNatu- positionaltext-to-imagesynthesis. TheEleventhIn- ralLanguageProcessing(EMNLP-IJCNLP),pages ternationalConferenceonLearningRepresentations. 3982â€“3992,HongKong,China.AssociationforCom- putationalLinguistics.\n\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\n\nSimCSE:Simplecontrastivelearningofsentenceem- RobinRombach,AndreasBlattmann,DominikLorenz, beddings. In Proceedings of the 2021 Conference Patrick Esser, and BjÃ¶rn Ommer. 2022. High- onEmpiricalMethodsinNaturalLanguageProcess- resolutionimagesynthesiswithlatentdiffusionmod- ing,pages6894â€“6910,OnlineandPuntaCana,Do- els. In Proceedings of the IEEE/CVF conference minican Republic. Association for Computational oncomputervisionandpatternrecognition, pages Linguistics. 10684â€“10695.\n\nPaulJaccard.1912. Thedistributionoftheflorainthe Chitwan Saharia, William Chan, Saurabh Saxena, alpinezone.1. Newphytologist,11(2):37â€“50.\n\nLala Li, Jay Whang, Emily L Denton, Kam- Tsung-YiLin,MichaelMaire,SergeBelongie,James yar Ghasemipour, Raphael Gontijo Lopes, Burcu Hays, Pietro Perona, Deva Ramanan, Piotr DollÃ¡r, Karagol Ayan, Tim Salimans, et al. 2022. Photo- and C Lawrence Zitnick. 2014. Microsoft coco: realistic text-to-image diffusion models with deep Common objects in context. In Computer Visionâ€“ languageunderstanding. AdvancesinNeuralInfor- ECCV 2014: 13th European Conference, Zurich, mationProcessingSystems,35:36479â€“36494.\n\nSwitzerland, September 6-12, 2014, Proceedings, Christoph Schuhmann, Romain Beaumont, Richard PartV13,pages740â€“755.Springer.\n\nVencu,CadeGordon,RossWightman,MehdiCherti, YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man- Theo Coombes, Aarush Katta, Clayton Mullis, dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, MitchellWortsman,etal.2022. Laion-5b: Anopen Luke Zettlemoyer, and Veselin Stoyanov. 2019. large-scaledatasetfortrainingnextgenerationimage- Roberta: A robustly optimized bert pretraining ap- text models. Advances in Neural Information Pro- proach. arXivpreprintarXiv:1907.11692. cessingSystems,35:25278â€“25294.\n\nChristophSchuhmann,RichardVencu,RomainBeau- mont,RobertKaczmarczyk,ClaytonMullis,Aarush Katta,TheoCoombes,JeniaJitsev,andAranKomat- suzaki. 2021. Laion-400m: Open dataset of clip- filtered400millionimage-textpairs. NeurIPSData- CentricAIWorkshop2021.\n\nHugoTouvron,ThibautLavril,GautierIzacard,Xavier Martinet,Marie-AnneLachaux,TimothÃ©eLacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and effi- cient foundation language models. arXiv preprint arXiv:2302.13971.\n\nMertYuksekgonul,FedericoBianchi,PratyushaKalluri, DanJurafsky,andJamesZou.2023. Whenandwhy vision-languagemodelsbehavelikebags-of-words, and what to do about it? In The Eleventh Interna- tionalConferenceonLearningRepresentations.",
      "blocks": [
        {
          "id": "1",
          "type": "HEADING1",
          "text": "CLIP í…ìŠ¤íŠ¸ ì¸ì½”ë” ê°œì„ í•˜ê¸°"
        },
        {
          "id": "2",
          "type": "TEXT",
          "text": "ì´ ì—°êµ¬ëŠ” CLIP ëª¨ë¸ì„ ë” ì¢‹ê²Œ ë§Œë“œëŠ” ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤."
        },
        {
          "id": "3",
          "type": "TEXT",
          "text": "CLIPì€ ê¸€ê³¼ ê·¸ë¦¼ì„ í•¨ê»˜ ì´í•´í•˜ëŠ” ì¸ê³µì§€ëŠ¥ì…ë‹ˆë‹¤."
        },
        {
          "id": "4",
          "type": "TEXT",
          "text": "í•˜ì§€ë§Œ ê°™ì€ ëœ»ì˜ ë‹¤ë¥¸ í‘œí˜„ì„ ì˜ ì´í•´í•˜ì§€ ëª»í•©ë‹ˆë‹¤."
        },
        {
          "id": "6",
          "type": "TEXT",
          "text": "í•˜ì§€ë§Œ CLIPì€ ì´ ë‘˜ì„ ë‹¤ë¥´ê²Œ ì´í•´í•©ë‹ˆë‹¤.",
          "blank": true
        },
        {
          "id": "7",
          "type": "TEXT",
          "text": "ì—°êµ¬ì§„ì€ ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë°©ë²•ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤."
        },
        {
          "id": "8",
          "type": "TEXT",
          "text": "ë°”ë¡œ íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤."
        },
        {
          "id": "10",
          "type": "TEXT",
          "text": "ì´ ë°©ë²•ìœ¼ë¡œ CLIPì„ í›ˆë ¨ì‹œì¼°ìŠµë‹ˆë‹¤."
        },
        {
          "id": "11",
          "type": "TEXT",
          "text": "ê·¸ ê²°ê³¼ ì„±ëŠ¥ì´ í¬ê²Œ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "12",
          "type": "TEXT",
          "text": "íŠ¹íˆ ë¹„ìŠ·í•œ ëœ»ì˜ ë¬¸ì¥ë“¤ì„ ë” ì˜ ì´í•´í•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "14",
          "type": "TEXT",
          "text": "ì´ ì—°êµ¬ëŠ” ì¸ê³µì§€ëŠ¥ì´ ì–¸ì–´ë¥¼ ë” ì˜ ì´í•´í•˜ë„ë¡ ë•ìŠµë‹ˆë‹¤.",
          "blank": true
        },
        {
          "id": "15",
          "type": "PAGE_IMAGE",
          "prompt": "9-13ì„¸ ë‚œë…ì¦ ì•„ë™ì„ ìœ„í•œ CLIP ëª¨ë¸ ê°œë… ì„¤ëª… ì´ë¯¸ì§€. ì»´í“¨í„°ê°€ ê¸€ê³¼ ê·¸ë¦¼ì„ í•¨ê»˜ ì´í•´í•˜ëŠ” ëª¨ìŠµì„ ê°„ë‹¨í•œ ì•„ì´ì½˜ìœ¼ë¡œ í‘œí˜„. ë°ì€ ìƒ‰ìƒ, ë§Œí™”í’, í…ìŠ¤íŠ¸ ê¸ˆì§€",
          "concept": "CLIP ëª¨ë¸ì˜ ê¸°ë³¸ ê°œë…",
          "alt": "ê¸€ê³¼ ê·¸ë¦¼ì„ í•¨ê»˜ ì´í•´í•˜ëŠ” ì¸ê³µì§€ëŠ¥",
          "url": "https://dyslexia-pdf.s3.ap-northeast-2.amazonaws.com/resources/2025/09/18/94fef25b-c319-4885-82df-aec8ff0fb7d8.svg"
        },
        {
          "id": "16",
          "type": "HEADING2",
          "text": "ì—°êµ¬ ë°©ë²•"
        },
        {
          "id": "17",
          "type": "TEXT",
          "text": "ì—°êµ¬ì§„ì€ ë‘ ë‹¨ê³„ë¡œ íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆë¥¼ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "18",
          "type": "TEXT",
          "text": "ì²« ë²ˆì§¸ ë‹¨ê³„ëŠ” ì›ë˜ ë¬¸ì¥ì„ ë‹¤ì‹œ ì“°ëŠ” ê²ƒì…ë‹ˆë‹¤."
        },
        {
          "id": "19",
          "type": "TEXT",
          "text": "ì¸í„°ë„·ì˜ ê·¸ë¦¼ ì„¤ëª…ì€ ì¢…ì¢… ë³µì¡í•©ë‹ˆë‹¤."
        },
        {
          "id": "21",
          "type": "TEXT",
          "text": "ê·¸ë˜ì„œ ì´ëŸ° ë³µì¡í•œ ì„¤ëª…ì„ ê°„ë‹¨í•˜ê²Œ ë°”ê¿‰ë‹ˆë‹¤."
        },
        {
          "id": "22",
          "type": "TEXT",
          "text": "ì¼ìƒì—ì„œ ì“°ëŠ” ì‰¬ìš´ ë§ë¡œ ê³ ì¹˜ëŠ” ê²ƒì…ë‹ˆë‹¤.",
          "blank": true
        },
        {
          "id": "23",
          "type": "TEXT",
          "text": "ë‘ ë²ˆì§¸ ë‹¨ê³„ëŠ” ë” ë‹¤ì–‘í•œ í‘œí˜„ì„ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤."
        },
        {
          "id": "24",
          "type": "TEXT",
          "text": "ì²« ë²ˆì§¸ ë‹¨ê³„ì—ì„œ ë§Œë“  ë¬¸ì¥ì„ ë˜ ë‹¤ì‹œ ë°”ê¿‰ë‹ˆë‹¤."
        },
        {
          "id": "26",
          "type": "TEXT",
          "text": "ì˜ˆë¥¼ ë“¤ì–´ 'ë’¤ì§‘ì„ ìˆ˜ ìˆëŠ”'ì„ 'ëŒë¦´ ìˆ˜ ìˆëŠ”'ìœ¼ë¡œ ë°”ê¿‰ë‹ˆë‹¤."
        },
        {
          "id": "27",
          "type": "TEXT",
          "text": "ì´ë ‡ê²Œ í•´ì„œ ê°™ì€ ëœ»ì˜ ë‹¤ì–‘í•œ ë¬¸ì¥ì„ ë§Œë“­ë‹ˆë‹¤."
        },
        {
          "id": "29",
          "type": "TEXT",
          "text": "ì´ 500ë§Œ ê°œì˜ ì˜ˆì‹œë¥¼ ë§Œë“¤ì–´ í›ˆë ¨ì— ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.",
          "blank": true
        },
        {
          "id": "30",
          "type": "PAGE_IMAGE",
          "prompt": "9-13ì„¸ ë‚œë…ì¦ ì•„ë™ì„ ìœ„í•œ íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆ ìƒì„± ê³¼ì • ì´ë¯¸ì§€. ì›ë˜ ë¬¸ì¥â†’ê°„ë‹¨í•œ ë¬¸ì¥â†’ë‹¤ì–‘í•œ í‘œí˜„ ìˆœì„œë¥¼ í™”ì‚´í‘œë¡œ ì—°ê²°. ë°ì€ ìƒ‰ìƒ, ë‹¨ê³„ë³„ êµ¬ë¶„, ë§Œí™”í’",
          "concept": "ë‘ ë‹¨ê³„ íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆ ìƒì„± ê³¼ì •",
          "alt": "ë¬¸ì¥ì„ ë‹¨ê³„ë³„ë¡œ ë°”ê¾¸ëŠ” ê³¼ì •",
          "url": "https://dyslexia-pdf.s3.ap-northeast-2.amazonaws.com/resources/2025/09/18/e020e9b6-428c-40ca-9af9-8948b5dee0cf.svg"
        },
        {
          "id": "31",
          "type": "HEADING2",
          "text": "í›ˆë ¨ ë°©ë²•"
        },
        {
          "id": "32",
          "type": "TEXT",
          "text": "ParaCLIPì„ ë§Œë“¤ê¸° ìœ„í•´ íŠ¹ë³„í•œ í›ˆë ¨ ë°©ë²•ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "34",
          "type": "TEXT",
          "text": "ì²« ë²ˆì§¸ëŠ” ê·¸ë¦¼ê³¼ íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆë¥¼ ì—°ê²°í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤."
        },
        {
          "id": "35",
          "type": "TEXT",
          "text": "ë‘ ë²ˆì§¸ëŠ” ì›ë˜ ë¬¸ì¥ê³¼ íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆë¥¼ ì—°ê²°í•©ë‹ˆë‹¤."
        },
        {
          "id": "36",
          "type": "TEXT",
          "text": "ì„¸ ë²ˆì§¸ëŠ” ì„œë¡œ ë‹¤ë¥¸ íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆë“¤ì„ ì—°ê²°í•©ë‹ˆë‹¤."
        },
        {
          "id": "37",
          "type": "TEXT",
          "text": "ì´ë ‡ê²Œ í•´ì„œ ë¹„ìŠ·í•œ ëœ»ì˜ ë¬¸ì¥ë“¤ì´ ê°€ê¹Œì›Œì§‘ë‹ˆë‹¤.",
          "blank": true
        },
        {
          "id": "39",
          "type": "TEXT",
          "text": "ì´ë¯¸ì§€ ì¸ì½”ë”ëŠ” ê·¸ëŒ€ë¡œ ë‘ì—ˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "40",
          "type": "TEXT",
          "text": "ì´ë ‡ê²Œ í•˜ë©´ í›ˆë ¨ ì‹œê°„ì„ í¬ê²Œ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "41",
          "type": "TEXT",
          "text": "ë˜í•œ ê¸°ì¡´ CLIPì˜ ì¢‹ì€ ì ë“¤ì„ ìœ ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "43",
          "type": "TEXT",
          "text": "ì´ ë°ì´í„°ì…‹ì—ëŠ” 4ì–µ ê°œì˜ ê·¸ë¦¼-ê¸€ ìŒì´ ë“¤ì–´ìˆìŠµë‹ˆë‹¤.",
          "blank": true
        },
        {
          "id": "45",
          "type": "PAGE_IMAGE",
          "prompt": "9-13ì„¸ ë‚œë…ì¦ ì•„ë™ì„ ìœ„í•œ CLIP í›ˆë ¨ ê³¼ì • ì´ë¯¸ì§€. ê·¸ë¦¼ê³¼ ê¸€ì´ ì—°ê²°ë˜ëŠ” ëª¨ìŠµì„ ê°„ë‹¨í•œ í™”ì‚´í‘œì™€ ì•„ì´ì½˜ìœ¼ë¡œ í‘œí˜„. ë°ì€ ìƒ‰ìƒ, ë§Œí™”í’, ë³µì¡í•˜ì§€ ì•Šê²Œ",
          "concept": "CLIP ëª¨ë¸ í›ˆë ¨ ê³¼ì •",
          "alt": "ì¸ê³µì§€ëŠ¥ì´ ê·¸ë¦¼ê³¼ ê¸€ì„ ì—°ê²°í•´ì„œ ë°°ìš°ëŠ” ëª¨ìŠµ",
          "url": "https://dyslexia-pdf.s3.ap-northeast-2.amazonaws.com/resources/2025/09/18/54a60e36-6b9b-4ee7-aed8-8d7adaed3af4.svg"
        },
        {
          "id": "46",
          "type": "HEADING2",
          "text": "ì‹¤í—˜ ê²°ê³¼"
        },
        {
          "id": "47",
          "type": "TEXT",
          "text": "ParaCLIPì˜ ì„±ëŠ¥ì„ ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í–ˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "48",
          "type": "TEXT",
          "text": "ê°€ì¥ ì¤‘ìš”í•œ í…ŒìŠ¤íŠ¸ëŠ” íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆ ê²€ìƒ‰ì´ì—ˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "49",
          "type": "TEXT",
          "text": "ê°™ì€ ëœ»ì˜ ë‹¤ë¥¸ ë¬¸ì¥ìœ¼ë¡œ ê·¸ë¦¼ì„ ì°¾ëŠ” í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤."
        },
        {
          "id": "51",
          "type": "TEXT",
          "text": "ì •í™•ë„ê°€ ìµœëŒ€ 5.6%ê¹Œì§€ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤.",
          "blank": true
        },
        {
          "id": "52",
          "type": "TEXT",
          "text": "ë‹¤ë¥¸ í…ŒìŠ¤íŠ¸ì—ì„œë„ ì¢‹ì€ ì„±ê³¼ë¥¼ ê±°ë‘ì—ˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "54",
          "type": "TEXT",
          "text": "ì´ í…ŒìŠ¤íŠ¸ëŠ” ê·¸ë¦¼ ì† ì‚¬ë¬¼ë“¤ì˜ ê´€ê³„ë¥¼ ì´í•´í•˜ëŠ” ëŠ¥ë ¥ì„ ë´…ë‹ˆë‹¤."
        },
        {
          "id": "55",
          "type": "TEXT",
          "text": "ì˜ˆë¥¼ ë“¤ì–´ 'ê°œê°€ ë‚˜ë¬´ ë’¤ì— ìˆë‹¤'ë¥¼ ì´í•´í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤."
        },
        {
          "id": "57",
          "type": "TEXT",
          "text": "ì´ëŠ” ì‚¬ë¬¼ì˜ íŠ¹ì§•ì„ ì–¼ë§ˆë‚˜ ì˜ ì•„ëŠ”ì§€ ë³´ëŠ” í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤."
        },
        {
          "id": "59",
          "type": "TEXT",
          "text": "ì´ í…ŒìŠ¤íŠ¸ëŠ” ë¬¸ì¥ë“¤ì´ ì–¼ë§ˆë‚˜ ë¹„ìŠ·í•œ ëœ»ì¸ì§€ íŒë‹¨í•©ë‹ˆë‹¤.",
          "blank": true
        },
        {
          "id": "60",
          "type": "PAGE_IMAGE",
          "prompt": "9-13ì„¸ ë‚œë…ì¦ ì•„ë™ì„ ìœ„í•œ ì„±ëŠ¥ ë¹„êµ ê·¸ë˜í”„ ì´ë¯¸ì§€. ParaCLIPê³¼ ê¸°ì¡´ CLIPì˜ ì„±ëŠ¥ì„ ë§‰ëŒ€ê·¸ë˜í”„ë¡œ ë¹„êµ. ë°ì€ ìƒ‰ìƒ, ê°„ë‹¨í•œ ë§‰ëŒ€ê·¸ë˜í”„, í…ìŠ¤íŠ¸ ìµœì†Œí™”",
          "concept": "ParaCLIP ì„±ëŠ¥ í–¥ìƒ ê²°ê³¼",
          "alt": "ìƒˆë¡œìš´ ëª¨ë¸ì´ ê¸°ì¡´ ëª¨ë¸ë³´ë‹¤ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ê·¸ë˜í”„",
          "url": "https://dyslexia-pdf.s3.ap-northeast-2.amazonaws.com/resources/2025/09/18/999f3890-d13a-4841-bf05-aa28f06300e9.svg"
        },
        {
          "id": "61",
          "type": "HEADING2",
          "text": "ë‹¤ë¥¸ ëª¨ë¸ê³¼ì˜ ë¹„êµ"
        },
        {
          "id": "62",
          "type": "TEXT",
          "text": "ì—°êµ¬ì§„ì€ ParaCLIPì„ ë‹¤ë¥¸ ëª¨ë¸ë“¤ê³¼ ë¹„êµí–ˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "63",
          "type": "TEXT",
          "text": "OpenAIì˜ ì›ë˜ CLIP ëª¨ë¸ê³¼ ë¹„êµí–ˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "66",
          "type": "TEXT",
          "text": "LaCLIPì´ë¼ëŠ” ìµœì‹  ëª¨ë¸ê³¼ë„ ê²½ìŸí–ˆìŠµë‹ˆë‹¤.",
          "blank": true
        },
        {
          "id": "67",
          "type": "TEXT",
          "text": "ëª¨ë“  ë¹„êµì—ì„œ ParaCLIPì´ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤."
        },
        {
          "id": "68",
          "type": "TEXT",
          "text": "íŠ¹íˆ íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆ ê´€ë ¨ ì‘ì—…ì—ì„œ í¬ê²Œ ì•ì„°ìŠµë‹ˆë‹¤."
        },
        {
          "id": "69",
          "type": "TEXT",
          "text": "ê¸°ì¡´ ì‘ì—…ë“¤ì—ì„œë„ ì„±ëŠ¥ì„ ìœ ì§€í•˜ê±°ë‚˜ ê°œì„ í–ˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "71",
          "type": "TEXT",
          "text": "í•œ ë¶„ì•¼ë§Œ ì˜í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì „ì²´ì ìœ¼ë¡œ ìš°ìˆ˜í•©ë‹ˆë‹¤.",
          "blank": true
        },
        {
          "id": "73",
          "type": "TEXT",
          "text": "ì„¸ ê°€ì§€ ì†ì‹¤ í•¨ìˆ˜ê°€ ëª¨ë‘ ì¤‘ìš”í•œ ì—­í• ì„ í–ˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "74",
          "type": "TEXT",
          "text": "íŠ¹íˆ ê·¸ë¦¼ê³¼ íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆë¥¼ ì—°ê²°í•˜ëŠ” ë¶€ë¶„ì´ í•µì‹¬ì´ì—ˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "75",
          "type": "PAGE_IMAGE",
          "prompt": "9-13ì„¸ ë‚œë…ì¦ ì•„ë™ì„ ìœ„í•œ ëª¨ë¸ ë¹„êµ ì´ë¯¸ì§€. ì—¬ëŸ¬ ì¸ê³µì§€ëŠ¥ ëª¨ë¸ë“¤ì´ ê²½ì£¼í•˜ëŠ” ëª¨ìŠµì„ ë§Œí™”ë¡œ í‘œí˜„. ParaCLIPì´ 1ë“±ìœ¼ë¡œ ê²°ìŠ¹ì„ ì„ í†µê³¼. ë°ì€ ìƒ‰ìƒ, ì¹œê·¼í•œ ìºë¦­í„°",
          "concept": "ParaCLIPì˜ ìš°ìˆ˜í•œ ì„±ëŠ¥",
          "alt": "ParaCLIPì´ ë‹¤ë¥¸ ëª¨ë¸ë“¤ë³´ë‹¤ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ëª¨ìŠµ",
          "url": "https://dyslexia-pdf.s3.ap-northeast-2.amazonaws.com/resources/2025/09/18/87b5c546-3c91-43e6-9ff6-4abbc49f8ac9.svg"
        },
        {
          "id": "76",
          "type": "HEADING2",
          "text": "ì—°êµ¬ì˜ í•œê³„ì "
        },
        {
          "id": "77",
          "type": "TEXT",
          "text": "ParaCLIPì—ë„ ëª‡ ê°€ì§€ í•œê³„ì ì´ ìˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "78",
          "type": "TEXT",
          "text": "ë•Œë¡œëŠ” ê¸°ì¡´ ì‘ì—…ì—ì„œ ì„±ëŠ¥ì´ ì¡°ê¸ˆ ë–¨ì–´ì§€ê¸°ë„ í•©ë‹ˆë‹¤."
        },
        {
          "id": "80",
          "type": "TEXT",
          "text": "ì´ëŠ” ë°°ì¹˜ í¬ê¸°ì™€ ê´€ë ¨ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "81",
          "type": "TEXT",
          "text": "ë” í° ë°°ì¹˜ í¬ê¸°ë¥¼ ì‚¬ìš©í•˜ë©´ ê°œì„ ë  ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.",
          "blank": true
        },
        {
          "id": "83",
          "type": "TEXT",
          "text": "ì›ë˜ CLIPì€ í›¨ì”¬ í° ë°°ì¹˜ í¬ê¸°ë¡œ í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "84",
          "type": "TEXT",
          "text": "ì•ìœ¼ë¡œ ë” ë§ì€ ì—°êµ¬ê°€ í•„ìš”í•œ ë¶€ë¶„ì…ë‹ˆë‹¤."
        },
        {
          "id": "87",
          "type": "TEXT",
          "text": "ì´ëŸ° ë¶€ë¶„ë“¤ì€ ë¯¸ë˜ ì—°êµ¬ì—ì„œ ê°œì„ í•´ì•¼ í•  ê³¼ì œì…ë‹ˆë‹¤.",
          "blank": true
        }
      ]
    },
    {
      "page_number": 2,
      "original_content": "A ImplementationDetails intwolistsasfollows: In the data generation process, we used the |Lk âˆ©Lk| JS@k(L ,L ) := a b , (2) gpt-35-turbo-0301 model with the temper- a b |Lk âˆªLk| a b ature of 1.0 and top-p of 0.1. We paid approxi- mately 130 USD for using ChatGPT to generate where|Lk âˆªLk|isthecardinalityofthesetunion a b 300Kparaphrasesforcaptionsand300Kadditional betweenLk andLk. JS@kequals0whenLk and a b a paraphrasesforgeneratedparaphrases. Lk aredisjointandequals1whenLk andLk con- b a b We used the checkpoints of CLIP mod- tainthesameretrievalresults(althoughnotneces- els provided in the official OpenCLIP GitHub sarilyinthesameorder). Unliketheaverageover- repository.4 We used openai for Ope- lap, the Jaccard similarity does not assign more nAIâ€™s CLIP, laion400m_e32 for OpenCLIP weighttothehigher-rankedretrievalresults. (400M), laion2b_s34b_b79k for OpenCLIP C CaseStudy (2B),andlaion2b_s12b_b32kforOpenCLIP- RoBERTa. OurParaCLIPmodelsweretrainedfor Figure 3 shows several examples where our Par- oneepochusingtheAdamWoptimizer(Loshchilov aCLIP model yieled better retrieval results than and Hutter, 2019), coupled with a cosine anneal- OpenAIâ€™s CLIP for paraphrased queries. In the ingscheduler,oneightA10080GGPUs. Forfine- firstexample,theparaphrasedquery(queryB)con- tuning,alearningrateof5e-7,abatchsizeof3,072, tainedseveralsynonymssuchasâ€œpicture,â€â€œguy,â€ and a weight decay rate of 0.001 were used. All â€œcutting,â€andâ€œtiny,â€replacingthewordsâ€œimage,â€ reportedscoresweremeasuredonasinglerun. â€œman,â€â€œslicing,â€andâ€œsmall,â€respectively. While the CLIP model output dissimilar results for the B MetricsinParaphrasedRetrieval giventwoqueries,resultinginaperformancedrop forqueryB,ParaCLIPconsistentlyproducediden- Average overlap The top-k average overlap tical results for both queries. In the second ex- (AO@k) (Fagin et al., 2003) quantifies the rank ample, the only difference between the queries similarity between the top-k elements of the two wasthewordâ€œwas.â€ Despitethisminorvariation, lists. Let L and L be ordered lists of retrieved a b CLIP generated different sets of images. On the imagesfortwodifferentqueries. AO@kbetween other hand, ParaCLIP returned the same images the two lists is calculated based on the weighted for both queries and achieved a better recall for sumofintersectionsoftruncatedlistsasfollows: queryB,althoughtherecallscoreforqueryAwas AO@k(L a ,L b ) := 1 (cid:88) k |Ld a âˆ©Ld b | , (1) s p l l i e g , h q tl u y er lo y w B er w th as an cr t e h a a t t e o d f b C y L e I x P p . a In nd th in e g la th st e e s x h a o m rt - k d d=1 queryAintolongerexpressions. Forinstance,the concisephraseâ€œaremotecontrolâ€wastransformed where Ld = L [1 : d] and Ld = L [1 : d] repre- a a b b intothemoreelaboratephraseâ€œacontrollerfora sent the truncated lists at depth d and |Ld âˆ© Ld| a b televisionthatiswirelesslyoperated.â€ WhileCLIP indicatesthecardinalityofthesetintersectionbe- exhibitedhighsensitivitytothislongparaphrased tweenthesetruncatedlists. WhenAO@kequals1, query,ParaCLIPdemonstratedgreaterrobustness, itmeansthatthetop-kelementsofL andL are a b resulting in more consistent results and superior exactlythesame. Conversely,whenAO@kequals recallscores. 0, it implies that there is no overlap whatsoever betweenthetop-kelementsofL andL . AO@k a b gives more weight to the higher-ranked retrieval results because they contribute to more terms in theoverallsummationcomparedtolower-ranked results.\n\nJaccardsimilarity Thetop-kJaccardsimilarity (JS@k)(Jaccard,1912)iscalculatedastheratioof theintersectiontotheunionofthetop-kelements 4https://github.com/mlfoundations/open_clip\n\nCLIP ParaCLIP (Ours) Query A:an image of a man slicing a small pizza Query B:A picture of a guy cutting a tiny pizza.\n\nQuery A: A boy standing in the grass with a frisbee.\n\nQuery B: A boy wasstanding in the grass with a frisbee.\n\nQuery A: A remote control on a wooden table in front of a television.\n\nQuery B: A controller for a television that is wirelessly operated and is placed on a flat surface made of wood that is in front of the television.\n\nFigure 3: Examples of retrieved images by the CLIP (Radford et al., 2021) and our ParaCLIP models for two different queries. Note that the queries are obtained from the paraphrased retrieval dataset, and query B is a paraphraseforqueryA.Thegoldimagesaredenotedbyaboldborder.",
      "blocks": [
        {
          "id": "1",
          "type": "HEADING1",
          "text": "êµ¬í˜„ ì„¸ë¶€ì‚¬í•­"
        },
        {
          "id": "2",
          "type": "TEXT",
          "text": "ë°ì´í„° ìƒì„± ê³¼ì •ì—ì„œ ìš°ë¦¬ëŠ” íŠ¹ë³„í•œ ëª¨ë¸ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "3",
          "type": "TEXT",
          "text": "gpt-35-turbo-0301 ëª¨ë¸ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.",
          "vocabularyAnalysis": [
            {
              "word": "ëª¨ë¸",
              "startIndex": 17,
              "endIndex": 19,
              "definition": "ì»´í“¨í„°ê°€ í•™ìŠµí•˜ì—¬ ë§Œë“  ì¸ê³µì§€ëŠ¥ í”„ë¡œê·¸ë¨",
              "simplifiedDefinition": "ì»´í“¨í„°ê°€ ë˜‘ë˜‘í•´ì§€ë„ë¡ ë§Œë“  í”„ë¡œê·¸ë¨",
              "examples": [
                "ChatGPTë„ í•˜ë‚˜ì˜ ëª¨ë¸ì´ì—ìš”"
              ],
              "difficultyLevel": "medium",
              "reason": "ê¸°ìˆ  ìš©ì–´"
            }
          ]
        },
        {
          "id": "4",
          "type": "TEXT",
          "text": "ì˜¨ë„ëŠ” 1.0ìœ¼ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "5",
          "type": "TEXT",
          "text": "top-pëŠ” 0.1ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "6",
          "type": "TEXT",
          "text": "ChatGPTë¥¼ ì‚¬ìš©í•˜ëŠ” ë° ì•½ 130ë‹¬ëŸ¬ë¥¼ ì§€ë¶ˆí–ˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "7",
          "type": "TEXT",
          "text": "30ë§Œ ê°œì˜ ë¬¸ì¥ì„ ë‹¤ì‹œ ì“°ëŠ” ì‘ì—…ì„ í–ˆìŠµë‹ˆë‹¤.",
          "vocabularyAnalysis": [
            {
              "word": "íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆ",
              "startIndex": 0,
              "endIndex": 6,
              "definition": "ê°™ì€ ëœ»ì„ ë‹¤ë¥¸ ë§ë¡œ í‘œí˜„í•˜ëŠ” ê²ƒ",
              "simplifiedDefinition": "ê°™ì€ ë‚´ìš©ì„ ë‹¤ë¥¸ ë§ë¡œ ë°”ê¿” ì“°ëŠ” ê²ƒ",
              "examples": [
                "'í° ì§‘'ì„ 'ë„“ì€ ì§‘'ìœ¼ë¡œ ë°”ê¿” ì“°ëŠ” ê²ƒ"
              ],
              "difficultyLevel": "medium",
              "reason": "ì™¸ë˜ì–´"
            }
          ]
        },
        {
          "id": "8",
          "type": "TEXT",
          "text": "ì¶”ê°€ë¡œ 30ë§Œ ê°œì˜ ë¬¸ì¥ì„ ë” ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤.",
          "blank": true
        },
        {
          "id": "9",
          "type": "TEXT",
          "text": "ìš°ë¦¬ëŠ” CLIP ëª¨ë¸ì˜ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "10",
          "type": "TEXT",
          "text": "ì´ê²ƒì€ ê³µì‹ OpenCLIP GitHub ì €ì¥ì†Œì—ì„œ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.",
          "vocabularyAnalysis": [
            {
              "word": "ì €ì¥ì†Œ",
              "startIndex": 25,
              "endIndex": 28,
              "definition": "ì»´í“¨í„° í”„ë¡œê·¸ë¨ì„ ë³´ê´€í•˜ëŠ” ì˜¨ë¼ì¸ ê³µê°„",
              "simplifiedDefinition": "í”„ë¡œê·¸ë¨ì„ ì €ì¥í•´ë‘ëŠ” ì¸í„°ë„· ê³µê°„",
              "examples": [
                "GitHubëŠ” í”„ë¡œê·¸ë¨ ì €ì¥ì†Œì˜ˆìš”"
              ],
              "difficultyLevel": "medium",
              "reason": "ê¸°ìˆ  ìš©ì–´"
            }
          ]
        },
        {
          "id": "11",
          "type": "TEXT",
          "text": "ì—¬ëŸ¬ ì¢…ë¥˜ì˜ ëª¨ë¸ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "12",
          "type": "TEXT",
          "text": "OpenAIìš©ìœ¼ë¡œëŠ” openaië¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "13",
          "type": "TEXT",
          "text": "OpenCLIP 400Mìš©ìœ¼ë¡œëŠ” laion400m_e32ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "14",
          "type": "TEXT",
          "text": "OpenCLIP 2Bìš©ìœ¼ë¡œëŠ” laion2b_s34b_b79kë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "15",
          "type": "TEXT",
          "text": "OpenCLIP-RoBERTaìš©ìœ¼ë¡œëŠ” laion2b_s12b_b32kë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "16",
          "type": "PAGE_IMAGE",
          "prompt": "9-13ì„¸ ë‚œë…ì¦ ì•„ë™ì„ ìœ„í•œ AI ëª¨ë¸ í•™ìŠµ ê³¼ì • ì´ë¯¸ì§€. ì»´í“¨í„°ì™€ ë°ì´í„°, í•™ìŠµ ê³¼ì •ì„ ë‚˜íƒ€ë‚´ëŠ” ê°„ë‹¨í•œ ì•„ì´ì½˜ë“¤. ë°ì€ íŒŒë€ìƒ‰ê³¼ ì´ˆë¡ìƒ‰, ë§Œí™”í’ ìŠ¤íƒ€ì¼, í…ìŠ¤íŠ¸ ì—†ìŒ",
          "concept": "AI ëª¨ë¸ í•™ìŠµ ê³¼ì •",
          "alt": "ì»´í“¨í„°ê°€ ë°ì´í„°ë¡œ í•™ìŠµí•˜ëŠ” ëª¨ìŠµ",
          "url": "https://dyslexia-pdf.s3.ap-northeast-2.amazonaws.com/resources/2025/09/18/76b5ef6a-c1ce-451e-b316-00924a97f5e9.svg"
        },
        {
          "id": "17",
          "type": "TEXT",
          "text": "ìš°ë¦¬ì˜ ParaCLIP ëª¨ë¸ì€ í•œ ë²ˆì˜ í•™ìŠµ ê³¼ì •ì„ ê±°ì³¤ìŠµë‹ˆë‹¤.",
          "vocabularyAnalysis": [
            {
              "word": "ì—í¬í¬",
              "startIndex": 0,
              "endIndex": 3,
              "definition": "ì „ì²´ ë°ì´í„°ë¥¼ í•œ ë²ˆ í•™ìŠµí•˜ëŠ” ê³¼ì •",
              "simplifiedDefinition": "ëª¨ë“  ìë£Œë¥¼ í•œ ë²ˆ ê³µë¶€í•˜ëŠ” ê²ƒ",
              "examples": [
                "ì±…ì„ ì²˜ìŒë¶€í„° ëê¹Œì§€ í•œ ë²ˆ ì½ëŠ” ê²ƒê³¼ ê°™ì•„ìš”"
              ],
              "difficultyLevel": "hard",
              "reason": "ì „ë¬¸ ìš©ì–´"
            }
          ]
        },
        {
          "id": "18",
          "type": "TEXT",
          "text": "AdamW ìµœì í™” ë„êµ¬ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "19",
          "type": "TEXT",
          "text": "ì½”ì‚¬ì¸ ìŠ¤ì¼€ì¤„ëŸ¬ë„ í•¨ê»˜ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "20",
          "type": "TEXT",
          "text": "8ê°œì˜ A100 80G GPUë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.",
          "vocabularyAnalysis": [
            {
              "word": "GPU",
              "startIndex": 12,
              "endIndex": 15,
              "definition": "ê·¸ë˜í”½ ì²˜ë¦¬ ì¥ì¹˜, ì»´í“¨í„° ì—°ì‚°ì„ ë¹ ë¥´ê²Œ í•˜ëŠ” ë¶€í’ˆ",
              "simplifiedDefinition": "ì»´í“¨í„°ë¥¼ ë¹ ë¥´ê²Œ ë§Œë“œëŠ” íŠ¹ë³„í•œ ë¶€í’ˆ",
              "examples": [
                "ê²Œì„ì´ë‚˜ AI í•™ìŠµì— ì‚¬ìš©í•´ìš”"
              ],
              "difficultyLevel": "medium",
              "reason": "ê¸°ìˆ  ìš©ì–´"
            }
          ]
        },
        {
          "id": "21",
          "type": "TEXT",
          "text": "ì„¸ë°€í•œ ì¡°ì •ì„ ìœ„í•´ íŠ¹ë³„í•œ ì„¤ì •ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.",
          "blank": true
        },
        {
          "id": "22",
          "type": "TEXT",
          "text": "í•™ìŠµë¥ ì€ 5e-7ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "23",
          "type": "TEXT",
          "text": "ë°°ì¹˜ í¬ê¸°ëŠ” 3,072ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.",
          "vocabularyAnalysis": [
            {
              "word": "ë°°ì¹˜",
              "startIndex": 0,
              "endIndex": 2,
              "definition": "í•œ ë²ˆì— ì²˜ë¦¬í•˜ëŠ” ë°ì´í„°ì˜ ë¬¶ìŒ",
              "simplifiedDefinition": "í•œ ë²ˆì— ê³µë¶€í•˜ëŠ” ìë£Œì˜ ì–‘",
              "examples": [
                "í•œ ë²ˆì— 10ê°œì”© ë¬¸ì œë¥¼ í‘¸ëŠ” ê²ƒê³¼ ê°™ì•„ìš”"
              ],
              "difficultyLevel": "medium",
              "reason": "ê¸°ìˆ  ìš©ì–´"
            }
          ]
        },
        {
          "id": "24",
          "type": "TEXT",
          "text": "ê°€ì¤‘ì¹˜ ê°ì†Œìœ¨ì€ 0.001ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "25",
          "type": "TEXT",
          "text": "ëª¨ë“  ì ìˆ˜ëŠ” í•œ ë²ˆì˜ ì‹¤í–‰ìœ¼ë¡œ ì¸¡ì •í–ˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "26",
          "type": "HEADING2",
          "text": "ë‹¤ì‹œ ì“´ ë¬¸ì¥ ê²€ìƒ‰ì˜ ì¸¡ì • ë°©ë²•"
        },
        {
          "id": "27",
          "type": "TEXT",
          "text": "í‰ê·  ê²¹ì¹¨ ì •ë„ë¥¼ ì¸¡ì •í•˜ëŠ” ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "28",
          "type": "TEXT",
          "text": "ìƒìœ„ kê°œ í‰ê·  ê²¹ì¹¨(AO@k)ì´ë¼ê³  í•©ë‹ˆë‹¤.",
          "vocabularyAnalysis": [
            {
              "word": "ê²¹ì¹¨",
              "startIndex": 8,
              "endIndex": 10,
              "definition": "ë‘ ê°œ ì´ìƒì´ ê°™ì€ ë¶€ë¶„ì„ ê°€ì§€ëŠ” ê²ƒ",
              "simplifiedDefinition": "ê°™ì€ ê²ƒì´ ìˆëŠ” ì •ë„",
              "examples": [
                "ë‘ ëª…ë‹¨ì—ì„œ ê°™ì€ ì´ë¦„ì´ ë‚˜ì˜¤ëŠ” ê²ƒ"
              ],
              "difficultyLevel": "easy",
              "reason": "ê°œë… ì„¤ëª… í•„ìš”"
            }
          ]
        },
        {
          "id": "29",
          "type": "TEXT",
          "text": "ì´ê²ƒì€ ë‘ ëª©ë¡ì˜ ìˆœìœ„ ìœ ì‚¬ì„±ì„ ì¸¡ì •í•©ë‹ˆë‹¤."
        },
        {
          "id": "30",
          "type": "TEXT",
          "text": "Laì™€ Lbë¥¼ ì„œë¡œ ë‹¤ë¥¸ ì§ˆë¬¸ì˜ ê²€ìƒ‰ ê²°ê³¼ë¼ê³  í•©ì‹œë‹¤."
        },
        {
          "id": "31",
          "type": "PAGE_IMAGE",
          "prompt": "9-13ì„¸ ë‚œë…ì¦ ì•„ë™ì„ ìœ„í•œ ë‘ ëª©ë¡ ë¹„êµ ì´ë¯¸ì§€. ë‘ ê°œì˜ ìˆœì„œê°€ ìˆëŠ” ëª©ë¡ê³¼ ê²¹ì¹˜ëŠ” ë¶€ë¶„ì„ í™”ì‚´í‘œë¡œ í‘œì‹œ. ë°ì€ ìƒ‰ìƒ, ê°„ë‹¨í•œ ë„í˜•, ë§Œí™”í’",
          "concept": "ëª©ë¡ ë¹„êµì™€ ê²¹ì¹¨",
          "alt": "ë‘ ëª©ë¡ì—ì„œ ê°™ì€ í•­ëª©ì„ ì°¾ëŠ” ëª¨ìŠµ",
          "url": "https://dyslexia-pdf.s3.ap-northeast-2.amazonaws.com/resources/2025/09/18/a6badb39-c803-496c-b43c-878b8e5de0af.svg"
        },
        {
          "id": "32",
          "type": "TEXT",
          "text": "AO@këŠ” ì˜ë¦° ëª©ë¡ë“¤ì˜ êµì§‘í•©ì„ ê¸°ë°˜ìœ¼ë¡œ ê³„ì‚°ë©ë‹ˆë‹¤.",
          "vocabularyAnalysis": [
            {
              "word": "êµì§‘í•©",
              "startIndex": 15,
              "endIndex": 18,
              "definition": "ë‘ ì§‘í•©ì—ì„œ ê³µí†µìœ¼ë¡œ ë“¤ì–´ìˆëŠ” ì›ì†Œë“¤ì˜ ì§‘í•©",
              "simplifiedDefinition": "ë‘ ê·¸ë£¹ì—ì„œ ê°™ì€ ê²ƒë“¤ë§Œ ëª¨ì€ ê²ƒ",
              "examples": [
                "Aë°˜ê³¼ Bë°˜ì—ì„œ ëª¨ë‘ ì¢‹ì•„í•˜ëŠ” ìŒì‹ë“¤"
              ],
              "difficultyLevel": "medium",
              "reason": "ìˆ˜í•™ ìš©ì–´"
            }
          ]
        },
        {
          "id": "33",
          "type": "TEXT",
          "text": "ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ í•©ìœ¼ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤."
        },
        {
          "id": "34",
          "type": "TEXT",
          "text": "AO@kê°€ 1ì´ë©´ ìƒìœ„ kê°œ ìš”ì†Œê°€ ì™„ì „íˆ ê°™ë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤."
        },
        {
          "id": "35",
          "type": "TEXT",
          "text": "AO@kê°€ 0ì´ë©´ ê²¹ì¹˜ëŠ” ë¶€ë¶„ì´ ì „í˜€ ì—†ë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤."
        },
        {
          "id": "36",
          "type": "TEXT",
          "text": "AO@këŠ” ë†’ì€ ìˆœìœ„ì˜ ê²°ê³¼ì— ë” í° ê°€ì¤‘ì¹˜ë¥¼ ì¤ë‹ˆë‹¤.",
          "vocabularyAnalysis": [
            {
              "word": "ê°€ì¤‘ì¹˜",
              "startIndex": 15,
              "endIndex": 18,
              "definition": "ê°ê°ì˜ ì¤‘ìš”ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ìˆ˜ì¹˜",
              "simplifiedDefinition": "ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ìˆ«ì",
              "examples": [
                "ì‹œí—˜ì—ì„œ ìˆ˜í•™ì´ 50ì , ì˜ì–´ê°€ 30ì ì˜ ë¹„ì¤‘ì„ ê°€ì§€ëŠ” ê²ƒ"
              ],
              "difficultyLevel": "medium",
              "reason": "ìˆ˜í•™ ìš©ì–´"
            }
          ]
        },
        {
          "id": "37",
          "type": "TEXT",
          "text": "ë‚®ì€ ìˆœìœ„ë³´ë‹¤ ë†’ì€ ìˆœìœ„ê°€ ë” ì¤‘ìš”í•˜ê²Œ ê³„ì‚°ë©ë‹ˆë‹¤.",
          "blank": true
        },
        {
          "id": "38",
          "type": "TEXT",
          "text": "ìì¹´ë“œ ìœ ì‚¬ë„ë¼ëŠ” ë‹¤ë¥¸ ì¸¡ì • ë°©ë²•ë„ ìˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "39",
          "type": "TEXT",
          "text": "ìƒìœ„ kê°œ ìì¹´ë“œ ìœ ì‚¬ë„(JS@k)ë¼ê³  í•©ë‹ˆë‹¤.",
          "vocabularyAnalysis": [
            {
              "word": "ìì¹´ë“œ",
              "startIndex": 0,
              "endIndex": 3,
              "definition": "ì§‘í•©ì˜ ìœ ì‚¬ì„±ì„ ì¸¡ì •í•˜ëŠ” ë°©ë²•ì˜ ì´ë¦„",
              "simplifiedDefinition": "ë‘ ê·¸ë£¹ì´ ì–¼ë§ˆë‚˜ ë¹„ìŠ·í•œì§€ ì¬ëŠ” ë°©ë²•",
              "examples": [
                "ë‘ ë°˜ í•™ìƒë“¤ì´ ì–¼ë§ˆë‚˜ ë¹„ìŠ·í•œì§€ ë¹„êµí•˜ëŠ” ê²ƒ"
              ],
              "difficultyLevel": "hard",
              "reason": "ì „ë¬¸ ìš©ì–´"
            }
          ]
        },
        {
          "id": "40",
          "type": "TEXT",
          "text": "ì´ê²ƒì€ êµì§‘í•©ê³¼ í•©ì§‘í•©ì˜ ë¹„ìœ¨ë¡œ ê³„ì‚°ë©ë‹ˆë‹¤."
        },
        {
          "id": "41",
          "type": "TEXT",
          "text": "JS@këŠ” Laì™€ Lbê°€ ì™„ì „íˆ ë‹¤ë¥´ë©´ 0ì…ë‹ˆë‹¤."
        },
        {
          "id": "42",
          "type": "TEXT",
          "text": "JS@këŠ” ê°™ì€ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì§€ë©´ 1ì…ë‹ˆë‹¤."
        },
        {
          "id": "43",
          "type": "TEXT",
          "text": "ìˆœì„œê°€ ë‹¬ë¼ë„ ê°™ì€ ê²°ê³¼ë©´ 1ì´ ë©ë‹ˆë‹¤."
        },
        {
          "id": "44",
          "type": "TEXT",
          "text": "í‰ê·  ê²¹ì¹¨ê³¼ ë‹¬ë¦¬ ë†’ì€ ìˆœìœ„ì— ë” í° ê°€ì¤‘ì¹˜ë¥¼ ì£¼ì§€ ì•ŠìŠµë‹ˆë‹¤."
        },
        {
          "id": "45",
          "type": "HEADING2",
          "text": "ì‚¬ë¡€ ì—°êµ¬"
        },
        {
          "id": "46",
          "type": "PAGE_IMAGE",
          "prompt": "9-13ì„¸ ë‚œë…ì¦ ì•„ë™ì„ ìœ„í•œ ì‚¬ë¡€ ì—°êµ¬ ì´ë¯¸ì§€. ë‘ ê°œì˜ ë‹¤ë¥¸ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¹„êµí•˜ëŠ” ëª¨ìŠµ. ì»´í“¨í„° í™”ë©´ê³¼ ê²€ìƒ‰ ê²°ê³¼ë“¤, ë°ì€ ìƒ‰ìƒ, ë§Œí™”í’",
          "concept": "ê²€ìƒ‰ ê²°ê³¼ ë¹„êµ ì—°êµ¬",
          "alt": "ë‘ ê°€ì§€ ê²€ìƒ‰ ë°©ë²•ì˜ ê²°ê³¼ë¥¼ ë¹„êµí•˜ëŠ” ëª¨ìŠµ",
          "url": "https://dyslexia-pdf.s3.ap-northeast-2.amazonaws.com/resources/2025/09/18/75714365-fef6-471c-b9d7-de0e0f4ff2f4.svg"
        },
        {
          "id": "47",
          "type": "TEXT",
          "text": "ê·¸ë¦¼ 3ì€ ì—¬ëŸ¬ ì˜ˆì‹œë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤."
        },
        {
          "id": "48",
          "type": "TEXT",
          "text": "ìš°ë¦¬ì˜ ParaCLIP ëª¨ë¸ì´ ë” ì¢‹ì€ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤.",
          "vocabularyAnalysis": [
            {
              "word": "ê²€ìƒ‰",
              "startIndex": 20,
              "endIndex": 22,
              "definition": "ì›í•˜ëŠ” ì •ë³´ë‚˜ ìë£Œë¥¼ ì°¾ëŠ” ê²ƒ",
              "simplifiedDefinition": "ì°¾ê³  ì‹¶ì€ ê²ƒì„ ì°¾ëŠ” ê²ƒ",
              "examples": [
                "êµ¬ê¸€ì—ì„œ 'ê°•ì•„ì§€ ì‚¬ì§„'ì„ ì°¾ëŠ” ê²ƒ"
              ],
              "difficultyLevel": "easy",
              "reason": "ê°œë… ì„¤ëª… í•„ìš”"
            }
          ]
        },
        {
          "id": "49",
          "type": "TEXT",
          "text": "OpenAIì˜ CLIPë³´ë‹¤ ë‹¤ì‹œ ì“´ ì§ˆë¬¸ì—ì„œ ë” ì¢‹ì•˜ìŠµë‹ˆë‹¤."
        },
        {
          "id": "50",
          "type": "TEXT",
          "text": "ì²« ë²ˆì§¸ ì˜ˆì‹œë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤."
        },
        {
          "id": "51",
          "type": "TEXT",
          "text": "ë‹¤ì‹œ ì“´ ì§ˆë¬¸(ì§ˆë¬¸ B)ì—ëŠ” ì—¬ëŸ¬ ë™ì˜ì–´ê°€ ìˆì—ˆìŠµë‹ˆë‹¤.",
          "vocabularyAnalysis": [
            {
              "word": "ë™ì˜ì–´",
              "startIndex": 18,
              "endIndex": 21,
              "definition": "ëœ»ì´ ê°™ê±°ë‚˜ ë¹„ìŠ·í•œ ë‹¨ì–´ë“¤",
              "simplifiedDefinition": "ê°™ì€ ëœ»ì„ ê°€ì§„ ë‹¤ë¥¸ ë§ë“¤",
              "examples": [
                "'í¬ë‹¤'ì™€ 'ê±°ëŒ€í•˜ë‹¤'ëŠ” ë™ì˜ì–´ì˜ˆìš”"
              ],
              "difficultyLevel": "medium",
              "reason": "ì–¸ì–´í•™ ìš©ì–´"
            }
          ]
        },
        {
          "id": "52",
          "type": "TEXT",
          "text": "'picture', 'guy', 'cutting', 'tiny' ê°™ì€ ë‹¨ì–´ë“¤ì´ì—ˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "53",
          "type": "TEXT",
          "text": "ì´ê²ƒë“¤ì€ 'image', 'man', 'slicing', 'small'ì„ ëŒ€ì‹ í–ˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "54",
          "type": "TEXT",
          "text": "CLIP ëª¨ë¸ì€ ë‘ ì§ˆë¬¸ì— ëŒ€í•´ ë‹¤ë¥¸ ê²°ê³¼ë¥¼ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "55",
          "type": "TEXT",
          "text": "ê·¸ë˜ì„œ ì§ˆë¬¸ Bì˜ ì„±ëŠ¥ì´ ë–¨ì–´ì¡ŒìŠµë‹ˆë‹¤."
        },
        {
          "id": "56",
          "type": "TEXT",
          "text": "í•˜ì§€ë§Œ ParaCLIPì€ ë‘ ì§ˆë¬¸ ëª¨ë‘ì—ì„œ ê°™ì€ ê²°ê³¼ë¥¼ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "57",
          "type": "TEXT",
          "text": "ë‘ ë²ˆì§¸ ì˜ˆì‹œì—ì„œëŠ” ì‘ì€ ì°¨ì´ë§Œ ìˆì—ˆìŠµë‹ˆë‹¤.",
          "blank": true
        },
        {
          "id": "58",
          "type": "TEXT",
          "text": "ë‘ ì§ˆë¬¸ì˜ ìœ ì¼í•œ ì°¨ì´ëŠ” 'was'ë¼ëŠ” ë‹¨ì–´ì˜€ìŠµë‹ˆë‹¤."
        },
        {
          "id": "59",
          "type": "TEXT",
          "text": "ì´ëŸ° ì‘ì€ ë³€í™”ì—ë„ CLIPì€ ë‹¤ë¥¸ ì´ë¯¸ì§€ë“¤ì„ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "60",
          "type": "TEXT",
          "text": "ë°˜ë©´ ParaCLIPì€ ë‘ ì§ˆë¬¸ ëª¨ë‘ì—ì„œ ê°™ì€ ì´ë¯¸ì§€ë¥¼ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤.",
          "vocabularyAnalysis": [
            {
              "word": "ë°˜í™˜",
              "startIndex": 25,
              "endIndex": 27,
              "definition": "ê²°ê³¼ë¥¼ ëŒë ¤ì£¼ëŠ” ê²ƒ",
              "simplifiedDefinition": "ë‹µì„ ì£¼ëŠ” ê²ƒ",
              "examples": [
                "ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì„ ì£¼ëŠ” ê²ƒ"
              ],
              "difficultyLevel": "medium",
              "reason": "ê¸°ìˆ  ìš©ì–´"
            }
          ]
        },
        {
          "id": "61",
          "type": "PAGE_IMAGE",
          "prompt": "9-13ì„¸ ë‚œë…ì¦ ì•„ë™ì„ ìœ„í•œ ê²€ìƒ‰ ê²°ê³¼ ë¹„êµ ì´ë¯¸ì§€. ê°™ì€ ì§ˆë¬¸ì— ëŒ€í•œ ë‘ ê°€ì§€ ë‹¤ë¥¸ ë‹µë³€ ê²°ê³¼. ì»´í“¨í„° í™”ë©´, ì´ë¯¸ì§€ ê²€ìƒ‰ ê²°ê³¼ë“¤, ë°ì€ ìƒ‰ìƒ",
          "concept": "ê²€ìƒ‰ ê²°ê³¼ì˜ ì¼ê´€ì„± ë¹„êµ",
          "alt": "ê°™ì€ ì§ˆë¬¸ì— ëŒ€í•œ ë‹¤ë¥¸ ê²€ìƒ‰ ê²°ê³¼ë“¤",
          "url": "https://dyslexia-pdf.s3.ap-northeast-2.amazonaws.com/resources/2025/09/18/ac643b26-b214-44e7-bfb1-84f60d8e3ab2.svg"
        },
        {
          "id": "62",
          "type": "TEXT",
          "text": "ì§ˆë¬¸ Bì—ì„œ ë” ì¢‹ì€ ì¬í˜„ìœ¨ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "63",
          "type": "TEXT",
          "text": "ì§ˆë¬¸ Aì˜ ì¬í˜„ìœ¨ ì ìˆ˜ëŠ” ì•½ê°„ ë‚®ì•˜ì§€ë§Œ ë§ì…ë‹ˆë‹¤.",
          "vocabularyAnalysis": [
            {
              "word": "ì¬í˜„ìœ¨",
              "startIndex": 5,
              "endIndex": 8,
              "definition": "ì°¾ì•„ì•¼ í•  ê²ƒ ì¤‘ì—ì„œ ì‹¤ì œë¡œ ì°¾ì€ ê²ƒì˜ ë¹„ìœ¨",
              "simplifiedDefinition": "ì°¾ì•„ì•¼ í•  ê²ƒì„ ì–¼ë§ˆë‚˜ ì˜ ì°¾ì•˜ëŠ”ì§€ì˜ ë¹„ìœ¨",
              "examples": [
                "10ê°œë¥¼ ì°¾ì•„ì•¼ í•˜ëŠ”ë° 8ê°œë¥¼ ì°¾ìœ¼ë©´ ì¬í˜„ìœ¨ì€ 80%"
              ],
              "difficultyLevel": "hard",
              "reason": "ì „ë¬¸ ìš©ì–´"
            }
          ]
        },
        {
          "id": "64",
          "type": "TEXT",
          "text": "ë§ˆì§€ë§‰ ì˜ˆì‹œì—ì„œëŠ” ì§ˆë¬¸ì„ í™•ì¥í–ˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "65",
          "type": "TEXT",
          "text": "ì§ˆë¬¸ Aë¥¼ ë” ê¸´ í‘œí˜„ìœ¼ë¡œ ë°”ê¿¨ìŠµë‹ˆë‹¤."
        },
        {
          "id": "66",
          "type": "TEXT",
          "text": "ì˜ˆë¥¼ ë“¤ì–´ 'a remote control'ì„ í™•ì¥í–ˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "67",
          "type": "TEXT",
          "text": "'a controller for a television that is wirelessly operated'ë¡œ ë°”ê¿¨ìŠµë‹ˆë‹¤.",
          "vocabularyAnalysis": [
            {
              "word": "ë¬´ì„ ìœ¼ë¡œ",
              "startIndex": 0,
              "endIndex": 4,
              "definition": "ì„  ì—†ì´ ì „íŒŒë¥¼ í†µí•´ ì—°ê²°ë˜ëŠ” ë°©ì‹",
              "simplifiedDefinition": "ì¤„ ì—†ì´ ì—°ê²°ë˜ëŠ” ë°©ì‹",
              "examples": [
                "ì™€ì´íŒŒì´ì²˜ëŸ¼ ì„  ì—†ì´ ì—°ê²°ë˜ëŠ” ê²ƒ"
              ],
              "difficultyLevel": "medium",
              "reason": "ê¸°ìˆ  ìš©ì–´"
            }
          ]
        },
        {
          "id": "68",
          "type": "TEXT",
          "text": "CLIPì€ ì´ëŸ° ê¸´ ë‹¤ì‹œ ì“´ ì§ˆë¬¸ì— ë§¤ìš° ë¯¼ê°í–ˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "69",
          "type": "TEXT",
          "text": "ParaCLIPì€ ë” ê°•í•œ ê²¬ê³ ì„±ì„ ë³´ì—¬ì¤¬ìŠµë‹ˆë‹¤.",
          "vocabularyAnalysis": [
            {
              "word": "ê²¬ê³ ì„±",
              "startIndex": 12,
              "endIndex": 15,
              "definition": "ë³€í™”ì— í”ë“¤ë¦¬ì§€ ì•Šê³  ì•ˆì •ì ì¸ ì„±ì§ˆ",
              "simplifiedDefinition": "ë³€í™”ê°€ ìˆì–´ë„ í”ë“¤ë¦¬ì§€ ì•ŠëŠ” ì„±ì§ˆ",
              "examples": [
                "ë°”ëŒì´ ë¶ˆì–´ë„ ë„˜ì–´ì§€ì§€ ì•ŠëŠ” íŠ¼íŠ¼í•œ ì§‘ì²˜ëŸ¼"
              ],
              "difficultyLevel": "medium",
              "reason": "ì¶”ìƒì  ê°œë…"
            }
          ]
        },
        {
          "id": "70",
          "type": "TEXT",
          "text": "ê·¸ ê²°ê³¼ ë” ì¼ê´€ëœ ê²°ê³¼ë¥¼ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "71",
          "type": "TEXT",
          "text": "ê·¸ë¦¬ê³  ë” ì¢‹ì€ ì¬í˜„ìœ¨ ì ìˆ˜ë¥¼ ì–»ì—ˆìŠµë‹ˆë‹¤."
        },
        {
          "id": "72",
          "type": "PAGE_IMAGE",
          "prompt": "9-13ì„¸ ë‚œë…ì¦ ì•„ë™ì„ ìœ„í•œ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼ ì´ë¯¸ì§€. ë‘ ê°œì˜ ë§‰ëŒ€ê·¸ë˜í”„ë‚˜ ì°¨íŠ¸ë¡œ ì„±ëŠ¥ ì°¨ì´ë¥¼ ë³´ì—¬ì£¼ëŠ” ëª¨ìŠµ. ë°ì€ ìƒ‰ìƒ, ê°„ë‹¨í•œ ê·¸ë˜í”„, ë§Œí™”í’",
          "concept": "AI ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ",
          "alt": "ë‘ AI ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë¹„êµí•œ ê·¸ë˜í”„",
          "url": "https://dyslexia-pdf.s3.ap-northeast-2.amazonaws.com/resources/2025/09/18/2601514c-a941-44ad-8f07-a3df136a798a.svg"
        }
      ]
    }
  ]
}